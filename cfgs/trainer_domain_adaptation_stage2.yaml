accelerator: gpu
strategy: auto
devices: 1
num_nodes: 1
precision: 32-true

logger:
  class_path: pytorch_lightning.loggers.WandbLogger
  init_args:
    project: domain_adaptation_res50
    log_model: true

callbacks:
  - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    init_args:
      monitor: score_D_epoch        # <-- same style as train_loss_epoch
      mode: min
      save_top_k: 1
      save_last: true

fast_dev_run: false
max_epochs: 50
min_epochs: -1
max_steps: -1
min_steps: -1

# Stage-2 has no validation, but checkpointing still works
check_val_every_n_epoch: null
val_check_interval: null
limit_val_batches: 0
num_sanity_val_steps: 0

enable_checkpointing: true
enable_progress_bar: true
enable_model_summary: true
accumulate_grad_batches: 1

deterministic: warn
benchmark: false

# MUST be false for full training loop
inference_mode: false

reload_dataloaders_every_n_epochs: 0

# The root directory is the same as Stage-1
default_root_dir: /develop/results/
